\documentclass[]{article}
\usepackage{caption,subcaption,graphicx,float,url,amsmath,amssymb,amsthm,tocloft,cancel,thmtools}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}

%opening
\title{Statistical Mechanics}
\author{Simon Crase}

\begin{document}

\maketitle

\begin{abstract}
These are my notes from the Statistical Mechanics lectures from Leonard Susskind's Theoretical Minimum series.

Statistical mechanics is a branch of physics that applies probability theory to the study of the thermodynamic behavior of systems composed of a large number of particles. Statistical mechanics provides a framework for relating the microscopic properties of individual atoms and molecules to the macroscopic bulk properties of materials that can be observed in everyday life. Thus it explains thermodynamics as a result of the classical and quantum-mechanical descriptions of statistics and mechanics at the microscopic level.
\end{abstract}

\tableofcontents

\listoftheorems[ignoreall,onlynamed]

\section{Entropy and conservation of information}

Professor Susskind introduces statistical mechanics as one of the most universal subjects in modern physics in terms of it's ability to explain and predict natural phenomena.  He begins with a brief introduction to probability theory and then moves on to draw the connection between the concept of laws of motion as rules for updating states of a system, and the probability of being in a given state.  Proper laws of physics are reversible and therefore preserve the distinctions between states - i.e. information.  In this sense, the conservation of information is more fundamental that other physical quantities such as temperature or energy.

Professor Susskind then moves on to continuous systems and phase space, and Liouville's theorem.  The lecture concludes with the presentation of formulas for computing entropy, and some examples.

This lecture focuses on the law of conservation of energy, the $-1^{th}$ law of physics. 

\begin{thm}[Liouville's theorem]
	''In physics, Liouville's theorem, named after the French mathematician Joseph Liouville, is a key theorem in classical statistical and Hamiltonian mechanics. It asserts that the phase-space distribution function is constant along the trajectories of the systemâ€”that is that the density of system points in the vicinity of a given system point traveling through phase-space is constant with time. This time-independent density is in statistical mechanics known as the classical a priori probability.''
\end{thm}

\begin{align*}
S \triangleq \log(M) \numberthis \label{eq:entropy:simple}
\end{align*}

Proper laws of physics are reversible and therefore preserve the distinctions between states - i.e. information.  In this sense, the conservation of information is more fundamental that other physical quantities such as temperature or energy.  
\begin{itemize}
	\item $S$ conserved as long as we can follow in detail, e.g. if we can follow the motion of each molecule that is disturbed by Friction.
	\item $S$ increases as our ignorance goes up;
	\item $S$ depends on the system and our state of knowledge.
\end{itemize}

\begin{align*}
S \triangleq - \sum_{i=1}^{N} p_i \log{p_i} \numberthis \label{eq:entropy}
\end{align*}


In (\ref{eq:entropy:simple}) and (\ref{eq:entropy}) we use the logarithm so that entropy adds, rather than multiplies.

The First law of Thermodynamics is energy conservation. For a closed system:
\begin{align*}
\frac{dE}{dt} =& 0\\
\sum \frac{dE_i}{dt} =& 0
\end{align*}


\section{Temperature}

Professor Susskind presents the physics of temperature. Temperature is not a fundamental quantity, but is derived as the amount of energy required to add an incremental amount of entropy to a system.  As the energy of a system increases, the number of possible states of a system increases, which means that the entropy increases.  This is the concept behind the second law of thermodynamics, and implies that temperature is always positive.

Temperature is really a measure of Energy. So the energy of one molecule of an Ideal Gas is given by:
\begin{align*}
E =& \frac{3}{2} k_B t\text{, where:}\\
k_B =& \text{Boltzmann's constant}\\
T =& \text{temperature in Kelvin}
\end{align*}
 In natural units we define a new unit of temperature, $T=k_Bt$, so $k_B=1$.
 
 We want to redefine temperature in therms of Entropy and Energy, and then show that this new definition agrees with our everyday idea of temperature. Temperature is not a fundamental quantity, but is derived as the amount of energy required to add an incremental amount of entropy to a system.  As the energy of a system increases, the number of possible states of a system increases, which means that the entropy increases.  This is the concept behind the second law of thermodynamics, and implies that temperature is always positive.
 
 Our definition reduces to:
\begin{align*}
\Delta E =& \underbrace{\frac{\partial E}{\partial S}}_\text{1 Degree of T} \Delta S\text{,or }\\
dE =& T dS \numberthis \label{eq:T}
\end{align*}

Consider two systems, $A$ and $B$, joined by a narrow pipe. Theorem \ref{thm:heat:flow} shows that T captures our intuition about temperature: heat flows from a higher temperature to lower.

\begin{thm}[Heat Flow]\label{thm:heat:flow}
	Given:
	\begin{align*}
	dE_A + dE_B =& 0 \numberthis \label{eq:thm:dE}\\
	dS_A + dS_B>& 0 \numberthis \label{eq:thm:eS} \\
	dE_i =& T_i dS_i \numberthis \label{eq:thm:T} \\
	T_A>&T_B \numberthis \label{eq:thm:Tgt}
	\end{align*}
	then energy flows from A to B, i.e.
	\begin{align*}
	dE_A<0
	\end{align*}
\end{thm}
\begin{proof}
	\begin{align*}
	T_A dS_A =& dE_A \text{, from (\ref{eq:thm:T})}\\
	=& - dE_B \text{, from (\ref{eq:thm:dE})}\\
	=& - T_B dS_B  \text{, from (\ref{eq:thm:T})}\\
	\implies&\\
	T_A dS_A + T_A dS_B =& T_A dS_B -T_B dS_B\\
	\implies&\\
	T_A (dS_A + dS_B) =& (T_A  -T_B) dS_B\\
	\implies& \text{, using (\ref{eq:thm:eS}) and (\ref{eq:thm:Tgt})}\\
	dS_B >& 0\\
	\implies& \text{using (\ref{eq:thm:T})}\\
	dE_B >& 0\\
	\implies& \text{, using (\ref{eq:thm:dE})}\\
	dE_A<0
	\end{align*}
\end{proof}



\section{Maximizing entropy}

After reviewing the laws of thermodynamics, Professor Susskind begins the derivation of how energy states are distributed for a complex system system with many energy states.

As the number of particles in a system grows, the distribution of states clumps more tightly around a mean. This is the Maxwell-Boltzmann distribution. The derivation requires Stirling's approximation and Lagrange multipliers.

Number of arrangements $\frac{N!}{\prod_{i}n_i!}$. We want to maximize this subject to two constraints:
\begin{align*}
\sum_{i=1}^{N} n_i =& N \numberthis\label{eq:sum:probilities}\\
\sum_{i=1}^{N} E_i n_i =& E N \numberthis\label{eq:total:energy}
\end{align*}

We will find Lemma \ref{lemma:Stirling} useful.
\begin{lemma}[Stirling's approximation]\label{lemma:Stirling}
	\begin{align*}
	N! \approxeq& N^N e^{-N}\numberthis\label{eq:stirling}
	\end{align*}
\end{lemma}

\begin{proof}
	\begin{align*}
	\log(N!) =& \sum_{i=1}^{N}\log (i)\\
	\approxeq &\int_{1}^{N} \log(x) dx\\
	\approxeq& \big[x \log(x) - x \big]_1^N\\
	\approxeq& N\log(N)-N\\
	N! \approxeq& N^N e^{-N}
	\end{align*}
\end{proof}	

\begin{thm}[To maximize the number of arrangements we need to maximize the entropy.]
	To maximize the number of arrangements we need to maximize the entropy.
\end{thm}

\begin{proof}
	\begin{align*}
	C=&\frac{N!}{\prod_{i}n_i!}\\
	\approxeq& \frac{N^N e^{-N}}{\prod_{i}n_i^{n_i} e^{-\sum_{i=1}^{N}n_i}}\tag*{from Lemma \ref{lemma:Stirling}}\\
	\approxeq& \frac{N^N \bcancel{e^{-N}}}{\prod_{i}n_i^{n_i} \bcancel{e^{-N}}}\tag*{using (\ref{eq:sum:probilities})}\\
	\log(C) \approxeq & N \log(N) - \sum_{i=1}^{N} n_i \log(n_i)\\
	\approxeq& N \log(N) - \sum_{i=1}^{N} N p_i \log(N p_i)\\
	\approxeq& N \log(N) - \sum_{i=1}^{N} N p_i \big(\log(N) + \log(p_i))\big)\\
	\approxeq& N \log(N) -  N \log(N) \bcancel{\sum_{i=1}^{N} p_i}  - N \sum_{i=1}^{N}p_i \log(p_i)\tag*{using (\ref{eq:sum:probilities})}\\
	\approxeq& \bcancel{N \log(N)} - \bcancel{N \log(N)}  - N \sum_{i=1}^{N}p_i \log(p_i)\\
	\approxeq& NS \text{, using (\ref{eq:entropy})} \numberthis \label{eq:entropy:logC}
	\end{align*}
\end{proof}

\section{The Boltzmann distribution}

Professor Susskind completes the derivation of the Boltzman distribution of states of a system. This distribution describes a system in equilibrium and with maximum entropy. He derives the formulas for energy, entropy, temperature, and the partition function for this distribution. He then applies these general formulas to the example of an ideal gas.

To maximize S subject to constraints (\ref{eq:sum:probilities}) and (\ref{eq:total:energy}), we minimize -S, using Lagrange Multipliers, i.e., minimize
\begin{align*}
\sum_{i=1}^{N}p_i \log(p_i) + \alpha(\sum_{i=1}^{N} p_i-1) + \beta(\sum_{i=1}^{N} E_i p_i-E)
\end{align*}

Differentiating with respect to $p_i$:
\begin{align*}
\log(p_i)+1 + \alpha +\beta E_i=&0\text{, whence}\\
p_i =& e^{-(1+\alpha)} e^{-\beta E_i}\\
=& \frac{1}{Z} e^{-\beta E_i}\text{, where $Z\triangleq e^{(1+\alpha)}$} \numberthis\label{eq:maximize:entropy}
\end{align*}

From (\ref{eq:sum:probilities}) and (\ref{eq:maximize:entropy})
\begin{align*}
1 =& \frac{1}{Z}  \sum_{i=1}^{N} e^-{\beta E_i}\\
Z(\beta) =& \sum_{i=1}^{N} e^-{\beta E_i} \text{. $Z$ is known as the Partition Function.} \numberthis \label{eq:partition:function}
\end{align*}

\begin{thm}[$\beta$ is the inverse temperature]\label{thm:inverseT}
	\begin{align*}
		T =& \frac{1}{\beta}\numberthis\label{eq:inverseT}
	\end{align*}
\end{thm}

\begin{proof}
	From (\ref{eq:total:energy}) and (\ref{eq:maximize:entropy})
	\begin{align*}
	E =& \frac{1}{Z}  \sum_{i=1}^{N} e^{-\beta E_i} E_i \numberthis \label{eq:E}\\
	\frac{\partial Z}{\partial \beta} =& - \sum_{i=1}^{N} e^{-\beta E_i} E_i \numberthis \label{eq:dZ}\\
	E(\beta) =& - \frac{1}{Z} \frac{\partial Z}{\partial \beta} = - \frac{\partial \log(Z)}{\partial \beta} \numberthis \label{eq:E:beta}
	\end{align*}
	
	\begin{align*}
	S =& -\sum_{i=1}^{N} p_i \log(p_i)\\
	=& - \sum_{i=1}^{N} \frac{1}{Z} e^{- \beta E_i}\big[- \beta E_i -\log(Z)\big]\\
	=& \beta E +  \bcancel{\frac{1}{Z}} \log(Z)  \bcancel{\sum_{i=1}^{N} e^{-\beta E_i}}\\
	=& \beta E +   \log(Z)  \numberthis \label{eq:S_Z}
	\end{align*}
	
	\begin{align*}
	dS =& \beta dE + E d\beta + \frac{\partial \log(Z)}{\partial \beta} d\beta \text{, from (\ref{eq:S_Z})}\\
	=& \beta dE \text{, using (\ref{eq:E:beta})}\\
	=& \frac{1}{T} dE \text{, using (\ref{eq:T})}\text{, whence}\\
	T =& \frac{1}{\beta}
	\end{align*}
\end{proof}

Ideal Gas: assume molecules don't interact.
State: 3N coordinates $\{X_1,...X_{3N}\}$ and momenta $\{P_1,...P3N\}$
\begin{align*}
Z =& \int d^{3N}x d^{3N}p . e^{- \frac{\beta}{2m} \sum_{n=1}^{3N} p_n^2}\\
=& \frac{V^N}{N!} \int d^{3N}p . e^{- \frac{\beta}{2m} \sum_{n=1}^{3N} p_n^2}\text{Factorial is controversial}\\
=& \frac{V^N}{N!} \big[\int dp. e^{-\frac{\beta}{2m}p^2}\big]^{3N}\\
=& \frac{V^N}{N!} \big[\frac{2 m \pi}{\beta}\big]^{\frac{3N}{2}}\numberthis\label{eq:Z_beta}
\end{align*}
We now use Stirling's formula (\ref{eq:stirling}).
\begin{align*}
\frac{V^N}{N!} \approxeq& \big(\frac{eN}{N}\big)^N \\
=& \big(\frac{e}{\rho}\big)^N,\text{, and (\ref{eq:E:beta}) becomes}\\
Z \approxeq& \big(\frac{e}{\rho}\big)^N .  \big[\frac{2 m \pi}{\beta}\big]^{\frac{3N}{2}}
\end{align*}

\begin{align*}
\log(Z) =& - \frac{3N}{2} \log(\beta) + const\\
E =& - \frac{\partial \log(Z)}{\partial \beta}\numberthis \label{eq:E:Z}\\
=& \frac{3N}{2} T
\end{align*}


\section{Pressure of an ideal gas and fluctuations}

Professor Susskind derives the formula for the pressure of an ideal gas.  He begins by introducing the Helmholtz free energy, and the concept of adiabatic processes.  These concepts lead to the definition of pressure as the change of energy with volume at a fixed entropy, and then to the famous equation of state for an ideal gas:  $pV = NkT$.

Professor Susskind then moves on to define the concept of fluctuations., and in the process demonstrates once again that the power in statistical mechanics lies in the partition function and is derivatives.  The fluctuation of energy in a system leads to the definition of the heat capacity of the system and the specific heat of a given material.

\subsection{Pressure of an ideal gas}

Motion is isotropic, even near wall. We know this from an honest use of statistical mechanics. Give up intuition because it might not be correct: ''ride the mathematics. All very great physicists are master of thermodynamics.''

We want to determine the Equation of State.

From (\ref{eq:S_Z}) and Theorem \ref{thm:inverseT}.
\begin{align*}
S =& \frac{E}{T} + \log(Z)\\
\underbrace{E - TS}_{A\triangleq\text{Helmholz Free Energy}} =& -T \log(Z) \numberthis \label{eq:helmholz}
\end{align*}

Variables come in pairs, e.g. (V,P):
\begin{itemize}
	\item Control Parameter (V);
	\item Conjugate Thermodynamic Variable (V).
\end{itemize}

\begin{lemma}[Derivatives along contour]\label{thm:derivative:contour}
	Given two dependent variables, S and E, say, and two independent variables, T and V, say:
	\begin{align*}
		\frac{\partial E}{\partial V}\bigg|_S = \frac{\partial E}{\partial V}\bigg|_T - 	\frac{\partial E}{\partial S}\bigg|_V \frac{\partial S}{\partial V}\bigg|_T
	\end{align*}
\end{lemma}
\begin{proof}
	Along a contour of fixed S:
	\begin{align*}
		\Delta E =& \frac{\partial E}{\partial V} \bigg|_T \Delta V + \frac{\partial E}{\partial T} \bigg|_V \Delta T\\
		\frac{\partial E}{\partial V} \bigg|_S=& \frac{\partial E}{\partial V} \bigg|_T  + \frac{\partial S}{\partial T} \bigg|_V \frac{\partial E}{\partial S} \bigg|_V \frac{\Delta T}{\Delta V} \numberthis\label{eq:dEdV}\\
		\text{Now }\Delta S =& \frac{\partial S}{\partial V} \bigg|_T \Delta V + \frac{\partial S}{\partial T} \bigg|_V \Delta T\\
		=& 0\text{, along a contour of constant S, whence}\\
		\frac{\Delta T}{\Delta V} =& - \Bigg[\frac{\frac{\partial S}{\partial V}\big|_T}{\frac{\partial S}{\partial T}\big|_V}\Bigg]\text{, along a contour of constant S, and (\ref{eq:dEdV}) becomes:}\\
		\frac{\partial E}{\partial V} \bigg|_S=& \frac{\partial E}{\partial V} \bigg|_T  - \cancel{\frac{\partial S}{\partial T} \bigg|_V} \frac{\partial E}{\partial S} \bigg|_V \frac{\frac{\partial S}{\partial V}\big|_T}{\cancel{\frac{\partial S}{\partial T}\big|_V}}\\
		=& \frac{\partial E}{\partial V} \bigg|_T  -  \frac{\partial E}{\partial S} \bigg|_V \frac{\partial S}{\partial V}\bigg|_T
	\end{align*}
\end{proof}

NB. For most situations, S is a monotonic increasing function of E. As E increases, distribution gets broader, and S increases. 

Imagine piston is vertical.
\begin{itemize}
	\item Move piston out, gas does work, which is paid for from energy.
	\item Move piston in, we do work, which adds to energy.
\end{itemize}

Move piston slowly, and don't let energy in. Adiabatic: slowly, and no heat in or out. $dE = - P dV$

\begin{align*}
\frac{\partial E}{\partial V}\bigg|_S =& - P\numberthis \label{eq:defP}
\end{align*}


\begin{thm}[Adiabatic Theorem]\label{thm:adiabatic}
	Change volume adiabatically, the system will ride along the energy level: it will change, but not jump.
\end{thm}

One consequence of Theorem \ref{thm:adiabatic} is that the $p_i$ are fixed.

Isentropic is also adiabatic.

Easier to use T as it appears in Bolzmann.

\begin{align*}
P =& - \frac{\partial E}{\partial V}\bigg|_S \\
=& - \frac{\partial E}{\partial V}\bigg|_T + \frac{\partial S}{\partial V}\bigg|_T \frac{\partial E}{\partial S}\bigg|_V\text{, from Lemma \ref{thm:derivative:contour}}\\
=& - \frac{\partial E}{\partial V}\bigg|_T + \frac{\partial S}{\partial V}\bigg|_T T\text{, from (\ref{eq:T})}\\
=& - \frac{\partial(E-TS)}{\partial V}\bigg|_T \text{, from (\ref{eq:helmholz})}\\
=& T \frac{\partial \log(Z)}{\partial V}\bigg|_T \numberthis \label{eq:P_Z_V}
\end{align*}

\begin{itemize}
	\item We haven't used assumption that we are dealing with a gas, so this is completely general.
	\item could have used different conjugate pair.
\end{itemize}

The partition function is given by:
\begin{align*}
Z(\beta) =& \int dx dp e^{- \beta \frac{p^2}{2m}}\\
=& \frac{V^N}{N!} f(\beta)\\
\log(Z) =& N \log(V) + \log(f(\beta))\text{, so (\ref{eq:P})  becomes:}\\
P =& \frac{NT}{V}\text{, or}\\
P =& \rho T
\end{align*}

\subsection{Fluctuations}

\begin{align*}
	\langle E -  \langle E \rangle \rangle^2 =& \langle E^2 \rangle - \langle E \rangle^2\text{. Now, from (\ref{eq:E:beta})}\\
	\langle E \rangle=& - \frac{\partial \log(Z)}{\partial \beta}\text{, and from (\ref{eq:dZ})}\\
	 \langle E^2 \rangle =& \frac{1}{Z} \frac{\partial^2 Z}{\partial \beta^2} \text{, whence}\\
	 \langle E^2 \rangle =& \frac{1}{Z} \frac{\partial^2 Z}{\partial \beta^2} - \frac{1}{Z^2} (\frac{\partial Z}{\partial \beta})^2 \numberthis \label{E2Z}
\end{align*}
All of statistical mechanics is about the power of the partition function.

\begin{align*}
	\frac{\partial^2 \log(Z)}{\partial \beta^2} =& \frac{\partial}{\partial \beta} \frac{1}{Z} \frac{\partial Z}{\partial \beta}\\
	=& \frac{1}{Z} \frac{\partial^2 Z}{\partial \beta^2} - \frac{1}{Z^2} (\frac{\partial Z}{\partial \beta})^2\\
	(\Delta E)^2 =& \frac{\partial}{\partial \beta} \frac{1}{Z} \frac{\partial Z}{\partial \beta}\text{, from (\ref{E2Z})}\\
	=& - \frac{\partial}{\partial \beta} \langle E \rangle\\
	=&  \underbrace{\frac{\partial}{\partial T} \langle E \rangle}_{C_V=^\text{ heat capacity}} T^2\\
	=& C_V T^2
\end{align*}

\begin{itemize}
	\item Einstein? Gibbs?
	\item True for any system: we haven't assumed ideal gas.
\end{itemize}


\section{Weakly interacting gases, heat, and work}

Professor Susskind derives the equations for the energy and pressure of a gas of weakly interacting particles, and develops the concepts of heat and work which lead to the first law of thermodynamics.

\subsection{Weakly interacting gases}

We will assume:
\begin{itemize}
	\item dilute;
	\item molecules on average far from each other;
	\item range of forces small compared to distance between particles;
	\item potential energy or forces small.
\end{itemize}

Then we will ask where this breaks down; where does correction becomes as large as uncorrected. 

We calculate partition function for a system where total energy not equal to sum of kinetic energies.

Assume energy has the following form:
\begin{align*}
E =& \sum_{n} \frac{p^2}{2M} + \underbrace{\sum_{n>m}u(\vert(x_m-x_n)\vert)}_{U(x)\text{, say}}
\end{align*}
, where $u$ is small compared to kinetic energy, and define:
\begin{align*}
U_0 \triangleq& \int d^3x x.u(\vert x \vert)\text{, then}\\
\int d^3x_1 d^3s_2 u(\vert x_1 - x_2\vert) = & V U_0\text{, ignoring surface expects!}
\end{align*}
NB: $U$ factors in collisions!

\begin{align*}
Z =& \int \frac{dp dx}{N!} e^{- \beta \frac{p^2}{2M}} e^{-\beta U(x)}\\
=&\underbrace{ \int \frac{dp}{N!} e^{- \beta \frac{p^2}{2M}}}_{\Big(\sqrt{\frac{2\pi m}{\beta}}\Big)^3} \int dx.e^{-\beta U(x)}\\
=&\underbrace{ \int \frac{dp.V^N}{N!} e^{- \beta \frac{p^2}{2M}}}_{\text{Partition function for Ideal Gas}} \int dx.\frac{e^{-\beta U(x)}}{V^N}\\
=&Z_0(\beta) \int dx.\frac{e^{-\beta U(x)}}{V^N}\text{ say.}\numberthis \label{eq:introducingZ0}\\
\int dx.\frac{e^{-\beta U(x)}}{V^N}\approxeq& \int \frac{dx}{V^N} \Big(1 - \beta U(x)\Big)\text{, assuming U small}\\
\approxeq & 1 - \beta \int \frac{dx}{V^N} U(x)\\
\approxeq & 1 - \beta \int \frac{dx}{V^N} \sum_{n>m} u(\vert x_n - x_m\vert)\\
\approxeq & 1 - \frac{N(N-1)\beta}{2} \int \frac{dx_1.dx_2}{V^N}  u(\vert x_1 - x_2\vert) V^{N-2}\\
\approxeq& 1 - \frac{N^2}{2}\beta \frac{U_0}{V}\\
Z \approxeq& Z_0(\beta) \Big[1 - \frac{\beta N^2}{2V} U_0\Big]
\end{align*}

\begin{align*}
\log(Z) =& \log(Z_0) + \log\Big[1 - \frac{\beta N^2}{2V} U_0\Big] \\
\approxeq & \log(Z_0) - \frac{\beta N^2}{2V} U_0\\
E =& - \frac{\partial \log Z}{\partial \beta}\\
=& \frac{3}{2}NT + \frac{N^2}{2V}U_0\\
=& \Big[\frac{3}{2}T + \frac{ \rho}{2}U_0\Big] N
\end{align*}

Since E is difficult to measure, we'll also calculate P. From (\ref{eq:P_Z_V}).

\begin{align*}
P =& T \frac{\partial \log(Z)}{\partial V}\bigg|_T\\
=& \rho T + \frac{\rho^2}{2} U_0\text{, provided $\rho U_0 \ll T$, i.e. potential energy much less than kinetic energy.}
\end{align*}

\subsection{Heat \& Work}

Given $F(x,y)$
\begin{align*}
dF =& \frac{\partial F}{\partial x}dx + \frac{\partial F}{\partial y}dy\\
=& F_x dx + F_y dy\text{, say}\numberthis \label{eg:dF}
\end{align*}

\begin{thm}[$\frac{\partial^2 F}{\partial x \partial y} = \frac{\partial^2 F}{\partial y \partial x}$]\label{thm:2partial}
	\begin{align*}
	\frac{\partial^2 F}{\partial x \partial y} = \frac{\partial^2 F}{\partial y \partial x}
	\end{align*}
\end{thm}

From Theorem \ref{thm:2partial}, (\ref{eg:dF}) $\implies \frac{\partial F_x}{\partial y} = \frac{\partial F_y}{\partial x}$.

\begin{defn}{Exact differential Form}
	If $\frac{\partial F_x}{\partial y} = \frac{\partial F_y}{\partial x}$, we say that the differential form $dF = \frac{\partial F}{\partial x}dx + \frac{\partial F}{\partial y}dy$ is exact.
\end{defn}

Consider an adiabatic change to system of cylinder and piston.

\begin{align*}
dE =& -P dV \text{, change V only}\\
=& T dS \text{, hold V, add heat}\\
=& \underbrace{-P dV}_{dW} + \underbrace{T dS}_{dQ}\text {, change both. First Law of Thermodynamics}
\end{align*}

Is $dQ$ an exact differential?

\begin{align*}
dQ =& dE + P.dV\text{. If exact}\\
\frac{\partial Q}{\partial E}=&1, \frac{\partial Q}{\partial V} = P\\
\frac{\partial^2 Q}{\partial V \partial E} \ne& \frac{\partial^2 Q}{\partial E \partial V}\text{, so $dQ$ is not exact.}
\end{align*}

\section{Entropy vs. reversibility}

Professor Susskind begins the lecture with 2 examples: (1) deriving the speed of sound in an ideal  gas; and (2) a single harmonic oscillator in a heat bath.  The harmonic oscillator example leads to a discrepancy with empirical observation that can only be resolved through quantum mechanics.  At low temperatures relative to the first excited state of the oscillator, quantum mechanics suppresses the energy of the harmonic oscillator.  Through this mechanism, certain modes of oscillation are "frozen out" until the system reaches higher temperatures.  Einstein proposed this quantized effect in 1907, which is one of the theories that led to the development of quantum mechanics.

Professor Susskind then discusses the apparent contradiction between the second law of thermodynamics, and the reversibility of classical mechanics.  If entropy always increases, reversibility is violated.  The resolution of this conflict lies in the (lack of) precision of our observations.  Undetectable differences in initial conditions lead to large changes in results.  This is the foundation of chaos theory.

\subsection{The speed of sound in an ideal gas}

Create a small overdense region. How fast does it move out? With the average velocity of molecules.

In thermal equilibrium in a dilute gas, every molecule has velocity
\begin{align*}
\frac{3}{2} k_B T =& \frac{m}{2} v^2\text{ We'll use laboratory units, so $k_B$}\\
v^2 =& \frac{3 k_B T}{m}\text{. A more detailed analysis gives:}\\
c^2 =& \frac{\partial P}{m \partial \rho}\\
P =& \rho k_B T\text{ Ideal gas law}\\
\implies&\\
c^2 =& \frac{k_B T}{m}\text{ Within order of magnitude!}
\end{align*}


\subsection{A single harmonic oscillator in a heat bath}

\begin{align*}
Energy =& \frac{m}{2} \dot{x}^ 2 + \frac{k}{2} x^2\tag{ Not Bolzmann!}\\
=& \frac{1}{2m} p^ 2 + \frac{k}{2} x^2\\
Z(\beta) =& \int e^{-\beta \frac{1}{2m} p^ 2 } e^{-\beta \frac{k}{2} x^2} dp dx\\
=& \int e^{-\beta \frac{1}{2m} p^ 2 } dp \int e^{-\beta \frac{k}{2} x^2} dx\\
=& \sqrt{\frac{2m\pi}{\beta}}\sqrt{\frac{2 \pi}{\beta k}}\\
=& \frac{2 \pi}{\omega}\frac{1}{\beta}\text{, where $\omega=$ frequency}\numberthis\label{eq:Z:classical:q}\\
\log(Z) =& const - \log(\beta)\\
E =& - \frac{\partial \log(Z)}{\partial \beta}\\
=& \frac{1}{\beta}\\
=& T
\end{align*}

\begin{itemize}
	\item Why no 3? Because 1 dimensional.
	\item Why not 2? Because 2 integrals.
\end{itemize}

Note:
\begin{itemize}
	\item Energy doesn't depend on $m$;
	\item Energy doesn't depend on $k$.
\end{itemize}

This seems wrong (make $k$ very large). This was a big problem around the end of 19th century (what about diatomic molecules 3 becomes 5?) We have ignored quantum mechanics!

\begin{align*}
	Z(\beta) =& \sum_{n} e^{- \beta n \hbar \omega}\\
	 =& \sum_{n} \Big(e^{- \beta\hbar \omega}\Big)^n\\
	 =& \frac{1}{1 - e^{- \beta\hbar \omega}}\numberthis\label{Z_quantum_shm}\\
	 E =& - \frac{1}{Z} \frac{\partial Z}{\partial \beta}\\
	 \frac{\partial Z}{\partial \beta}=& \frac{- \hbar \omega e^{- \beta\hbar \omega}}{(1 - e^{- \beta\hbar \omega})^2}\\
	 E =& \frac{ \hbar \omega e^{- \beta\hbar \omega}}{(1 - e^{- \beta\hbar \omega})}
\end{align*}

High temperature: $T\rightarrow\infty \implies \beta \rightarrow 0$
\begin{align*}
	\frac{ \hbar \omega e^{- \beta\hbar \omega}}{(1 - e^{- \beta\hbar \omega})}\rightarrow& \frac{\hbar \omega}{1 -(1 - \beta \hbar \omega...)}\\
	\rightarrow& \frac{\hbar \omega}{\beta \hbar \omega}\\
	\rightarrow& \frac{1}{\beta}\\
	\rightarrow& T\text{, which agrees with classical physics.}
\end{align*}

Low temperature: $T\rightarrow 0 \implies \beta \rightarrow \infty$

\begin{align*}
\frac{ \hbar \omega e^{- \beta\hbar \omega}}{(1 - e^{- \beta\hbar \omega})}\rightarrow&\hbar \omega e^{- \beta\hbar \omega}\\
\rightarrow&0
\end{align*}

So for low temperature, oscillation is suppressed.	Where is the crossover between classical and quantum behaviour? This occurs when exponential goes from large to small (and vice versa), i.e. when $\beta\hbar \omega \approx 1$.

\begin{itemize}
	\item $\beta\hbar \omega < 1$ Classical.
	\item $\beta\hbar \omega > 1$ Quantum.
	\item $\beta\hbar \omega = 1\equiv\hbar \omega=T$ Crossover. At this temperature, system has one quantum of energy; it doesn't want to have less!
	\item For very low temperature, diatomic molecule behaves like monoatomic.
	\item For a stiff molecule, $\omega$ is high, to crossover temperature is high.
\end{itemize}

c.f. Einstein specific heat.

Can we derive classical partition function from quantum?

\begin{align*}
Z(\beta) =&  \frac{1}{1 - e^{- \beta\hbar \omega}} \text{, from (\ref {Z_quantum_shm})}\\
\approxeq & \frac{1}{1 - 1 + \beta\hbar \omega}\text{, for high T}\\
\approxeq & \frac{1}{\beta\hbar \omega}\text{, c.f. (\ref{eq:Z:classical:q})}
\end{align*}

\begin{itemize}
	\item So the classical and quantum partition functions agree within a multiplicative constant, which disappears when we differentiate $\log(Z)$ to do anything useful.
	\item Heat system: it unfreezes degrees of freedom and we discover complexity. Cool system: discover quantum behaviour.
	\item Violin string: freeze out higher frequency states, so energy finite.
\end{itemize}

\subsection{Contradiction between the 2nd and reversibility}
\begin{itemize}
	\item Newton's laws reversible
	\item 2nd law
	\item Consider a small blob in phase space: according to Liouville's theorem, volume, and hence entropy, does not change! Correct view of microscopic system.
	\item We need to coarse grain. Resolution is limited, both by QM, and by our instruments.
	\item Imagine that blob turns into long thing snake (same volume), but we can't resolve, so we cover with blobs. We are now losing information. 
	\item Chaos! Sensitive dependence on initial conditions. Imagine phase space filled with cotton!
	
	\item Phase space trajectory will eventually come back to starting point within specified tolerance: time depends on tolerance.
	
	\item Entropy probably increases -- Boltzmann. 
\end{itemize}





\section{Entropy, reversibility, and magnetism}



Professor Susskind develops the equation for the probability that all molecules of a gas will converge in one half of a room, and concludes that this event is possible, but that the time scale for it to occur is incredibly long.  This line of reasoning leads to the resolution of the paradox between the reversibility of classical mechanics and the apparent lack of time reversibility of the second law of thermodynamics by demonstrating that statistical mechanics processes are in fact time reversible if the system is known precisely enough and the observer waits long enough.

He then moves on to magnetism and begins to introduce the concepts of ferromagnetic phase transitions and spontaneous symmetry breaking.  Spontaneous symmetry breaking occurs when magnets in a lattice begin to cool.  With no external magnetic field, they may end up in one of two symmetrical states - e.g. all up or all down.  But a very small magnetic field affecting just one of the magnets will break this symmetry and bias the system toward one of the ground states.

\subsection{2nd Law and Poincar\'e Recurrences}

Imagine room with all particles of air in one half. In 6N dimensional phase space, momentum is bounded because we have finite energy. Phase point moves chaotically. Phase point $\frac{1}{2^N}$. Or, imagine configuration space has N dimensions, and is of volume $V^N$; what is probability of finding system in smaller volume $v^N$? $\frac{v^N}{V^N}$. 

\begin{thm}[Poincar\'e Recurrences]
	Any dynamical system defined by an ordinary differential equation determines a flow map $f^t$ mapping phase space on itself. The system is said to be volume-preserving if the volume of a set in phase space is invariant under the flow. For instance, all Hamiltonian systems are volume-preserving because of Liouville's theorem. The theorem is then: If a flow preserves volume and has only bounded orbits, then for each open set there exist orbits that intersect the set infinitely often.
\end{thm}

Boltzmann's Brain: most of the ways we could have gotten here are by random fluctuation, and they don't have coherent histories.


\subsection{Magnetism}

\section{The Ising model}

After reviewing the discussion of a single magnetic particle (or spin) in a heat bath, Professor Susskind continues with the development of the one-dimensional Ising model.  This model does not exhibit phase transitions.  He then moves on to the multi-dimensional Ising model.  For any number of dimensions above one, the Ising model exhibits phase transitions at specific temperatures.  This is due to the fact that, in multiple dimensions, each point in the lattice is affected by more than two neighboring lattice elements.

\section{Liquid-gas phase transition}



Professor Susskind continues the discussion of phase transitions beginning with a review of the Ising model and the mean field approximation, and then presents the temperature and magnetic field parameters of the phase transition of a magnetic material. He then moves to the physics of the liquid-gas phase transition and develops the mathematical analogy between this case that that for a magnetic material.

Professor Susskind finishes the Theoretical Minimum series of courses by taking questions from the class, which leads to a discussion of the anthropic principle and fine tuning.

\end{document}
